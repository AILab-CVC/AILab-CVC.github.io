<!DOCTYPE html>
<html>

<head lang="en">
    <link rel="icon" type="image/png" href="./img/icon.png">

    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <style>
        iframe {
            height: 75vh;
            width: 100%;
            /* or any width you prefer */
            border: none;
        }
    </style>

    <title>Planting a SEED of Vision in Large Language Model</title>

    <meta name="description" content="SEED">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--FACEBOOK-->
    <meta property="og:image" content="./img/seed-1.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="2040">
    <meta property="og:image:height" content="1258">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="http://ailab-cvc.github.io/seed/seed.html"/>
    <meta property="og:title" content="Planting a SEED of Vision in Large Language Model" />
    <meta property="og:description"
        content="Project page for Planting a SEED of Vision in Large Language Model." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Planting a SEED of Vision in Large Language Model" />
    <meta name="twitter:description"
        content="Project page for Planting a SEED of Vision in Large Language Model." />
    <meta name="twitter:image" content="./img/seed-1.png" />


    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font.css">

    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-52J0PM8XKV');
    </script>


    <style>
        .nav-pills {
            position: relative;
            display: inline;
        }

        .author {
            position: relative;
            display: inline-block;
            border-bottom: 1px dotted black;
            /* If you want dots under the hoverable text */
        }

        /* Tooltip text */
        .author .affiliation {
            visibility: hidden;
            width: 120px;
            background-color: black;
            color: #fff;
            text-align: center;
            padding: 5px 0;
            border-radius: 6px;

            /* Position the tooltip text - see examples below! */
            position: absolute;
            z-index: 1;
            width: 120px;
            top: 100%;
            left: 50%;
            margin-left: -60px;
            /* Use half of the width (120/2 = 60), to center the tooltip */
        }

        /* Show the tooltip text when you mouse over the tooltip container */
        .author:hover .affiliation {
            visibility: visible;
        }

        .video-container {
            display: flex;
            flex-wrap: wrap;
            margin-top: 30px;
        }

        .video-wrapper {
            flex: 1;
            margin-right: 2px;
            margin-left: 2px;
            max-width: calc(33.33%px);
            /* 33.33% for 3 videos per row, subtracting margins */
            height: auto;
            text-align: center;
        }

        .video {
            max-width: 100%;
            height: auto;
        }

        .caption {
            margin-top: 0px;
        }

        .image-container {
            display: flex;
            flex-direction: row;
            /* Arrange items horizontally */
            justify-content: space-between;
            /* Spread items horizontally */
            align-items: flex-end;
            /* Align items at the bottom */
        }

        .image-container .image-wrapper {
            flex: 1;
            /* Distribute equal width to both images */
            padding: 10px;
            /* Add some spacing between images */
        }

        .image-container img {
            max-width: 100%;
            /* Constrain image width */
            height: auto;
            /* Maintain image aspect ratio */
            display: block;
            /* Remove extra space below inline images */
            margin: 0 auto;
            /* Center the images within their containers */
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong>
                  <font size="+6">Planting a <span class="rainbow-gradient">SEED</span> of Vision in Large Language Model</font>
            </h2>
        </div>

        <div class="row">

            <div class="col-md-12 text-center">

                <!-- <br>
                For any inquiries, please email <a href="mailto:seed-x@googlegroups.com">seed-x@googlegroups.com</a>
                <br> -->
                <ul class="list-inline">
                    <br>
                    <li class="author" data-affiliation="Tencent AI Lab"><a target="_blank" href="https://geyuying.github.io/">Yuying Ge*</a></li>
                    <li class="author" data-affiliation="Tencent ARC Lab, Tencent AI Lab"><a target="_blank" href="https://geyixiao.com/">Yixiao Ge*&#128231;</a></li>
                    <li class="author" data-affiliation="Tencent ARC Lab, Tsinghua University"><a target="_blank" href="https://stdkonjac.icu/">Ziyun Zeng</a></li>
                    <li class="author" data-affiliation="Tencent ARC Lab, Tencent AI Lab"><a target="_blank" href="https://xinntao.github.io/">Xintao Wang</a></li>
                    <li class="author" data-affiliation="Tencent ARC Lab, Tencent AI Lab"><a target="_blank" href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a></li>

                <br>
                    <a target="_blank" href="https://ai.tencent.com/">
                    <image src="img/ailab.png" height="37px"> </a>
                      &nbsp; &nbsp; &nbsp;
                    <a target="_blank" href="https://arc.tencent.com/">
                    <image src="img/ARC.svg" height="80px"> </a>
                <br>
                For any inquiries, please email <a href="mailto:seed-x@googlegroups.com">seed-x@googlegroups.com</a>
                <br><br>
                </ul>
            </div>


        </div>


        <div class="row">

            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">

                    <li>
                        <a target="_blank" href="https://arxiv.org/abs/2307.08041">
                            <image src="img/seed_1st_page.png" height="60px">
                                <h4><strong>Tech Report</strong></h4>
                        </a>
                    </li>

                    <li>
                        <a target="_blank" href="https://github.com/AILab-CVC/SEED">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code & Models</strong></h4>
                        </a>
                    </li>

                </ul>

            </div>

        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Updates</b>
                </h3>

                <p>
                  <span class="css-rainbow-text"><b>[2 Oct 2023]</b></span>&nbsp;
                  We release SEED-LLaMA equipped with the SEED-2 tokenizer.
                  Please refer to the <a target="_blank" href="./seed_llama.html">new page</a>.
                </p>

                <p>
                  <b>[29 July 2023]</b>&nbsp;
                  We release the checkpoint of the SEED tokenizer and its inference code on
                  <a target="_blank" href="https://github.com/AILab-CVC/SEED">GitHub</a>.
                </p>

                <p>
                  <b>[16 July 2023]</b>&nbsp;
                  Our technical report has been released on
                  <a target="_blank" href="https://arxiv.org/abs/2307.08041">arXiv</a>.
                </p>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Preface</b>
                </h3>
                <!-- <img src="img/seed_overview.jpg" class="img-responsive"></img> -->

                <p>
                  We present <span class="css-rainbow-text">SEED</span>, an elaborate image tokenizer that empowers Large Language Models (LLMs) with the emergent ability to SEE and Draw at the same time. Research on image tokenizers has previously reached an impasse, as frameworks employing quantized visual tokens have lost prominence due to subpar performance and convergence in multimodal comprehension (compared to BLIP-2, etc.) or generation (compared to Stable Diffusion, etc.). Despite the limitations, we remain confident in its natural capacity to unify visual and textual representations, facilitating scalable multimodal training with LLM's original recipe.
                </p>


            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b><span class="css-rainbow-text">SEED</span> Tokenizer</b>
                </h3>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="img/seed-1.png" class="img-responsive">
                    </div>
                </div>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <div style="text-align: center; margin-top: 0px; margin-bottom: 24px; font-size: 17px;" class="seed">
                    Overview of <span class="css-rainbow-text">SEED</span> tokenizer.
                </div>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <p>
                  We identify <span style="background-color: rgb(131, 253, 131)">two crucial principles</span>
                  for the architecture and training of SEED that effectively ease subsequent alignment with LLMs.
                  <ul>
                  <li><b>Causal Q-Former: </b>
                    Image tokens should be independent of 2D physical patch positions and
                    instead be produced with a <span style="background-color: rgb(169, 156, 226)">1D causal dependency</span>, exhibiting intrinsic
                    interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs.
                  </li>
                  <li><b>Contrastive and reconstruction learning targets: </b>
                    Image tokens should capture <span style="background-color: rgb(169, 156, 226)">high-level semantics</span>
                    consistent with the degree of semantic abstraction in words, and <span style="background-color: rgb(169, 156, 226)">be optimized for both discriminativeness
                    and reconstruction</span> during the tokenizer training phase.
                  </li>
                </ul>
                </p>
                <br>
                <p>
                  <span style="background-color: rgb(131, 253, 131)">The working mechanism of SEED:</span>
                  <ul>
                  <li><b>Tokenize: </b>
                    Causal Q-Former converts 2D raster-ordered features produced by
                    the ViT encoder into a sequence of causal semantic embeddings,
                    which are further discretized by the VQ Codebook.
                  </li>
                  <li><b>De-Tokenize: </b>
                    The discrete visual codes are decoded into generation embeddings
                    via Reverse Q-Former. The generation embeddings are aligned
                    with the latent space of SD so that realistic images with
                    consistent semantics can be generated using
                    the off-the-shelf SD-UNet.
                  </li>
                </ul>
                Note that the ViT encoder and UNet decoder are directly derived from
                the pre-trained BLIP-2 and SD models, respectively.
                There are 8192 visual codes in SEED, and each image is embedded into 32 tokens.
                </p>
            </div>

            <div class="col-md-8 col-md-offset-2">
                <br>
                <h4>
                    Evaluate the discriminativeness of <span class="css-rainbow-text">SEED</span> tokens
                </h4>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="img/seed-1-eval-retrieval.png" class="img-responsive">
                    </div>
                </div>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div style="text-align: center; margin-top: 0px; margin-bottom: 24px; font-size: 17px;" class="seed">
                    Evaluation of zero-shot image-text retrieval. Causal codes are quantized causal embeddings.
                </div>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <!-- <br> -->
                <h4>
                    Evaluate the reconstruction ability of <span class="css-rainbow-text">SEED</span> tokens
                </h4>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="img/seed-1-eval-gen.png" class="img-responsive">
                    </div>
                </div>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div style="text-align: center; margin-top: 0px; margin-bottom: 24px; font-size: 17px;" class="seed">
                  Reconstruction images of SEED tokenizer (i.e., original image → SEED tokenize → causal
                  visual codes → SEED de-tokenize → reconstructed image), which are semantically consistent with
                  the original input images.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b><span class="css-rainbow-text">SEED</span> for LLM</b>
                </h3>
                <p>
                  As a result, the off-the-shelf LLM is able to perform both image-to-text and text-to-image generation by incorporating our SEED through efficient LoRA tuning.
                  <!-- Comprehensive multimodal pretraining and instruction tuning, which may yield improved results, are reserved for future investigation. -->
                  We present SEED-OPT<sub>2.7B</sub>, which was trained in only 44 hours using 64 V100 GPUs and 5M image-caption pairs.
                  <!-- This version of SEED was trained in 5.7 days using only 64 V100 GPUs and 5M publicly available image-text pairs. -->
                  <!-- Our preliminary study emphasizes the great potential of discrete visual tokens in versatile multimodal LLMs and the importance of proper image tokenizers in broader research. -->
                </p>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="img/seed-opt.png" class="img-responsive">
                    </div>
                </div>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div style="text-align: center; margin-top: 0px; margin-bottom: 24px; font-size: 17px;" class="seed">
                  Overview of <span class="css-rainbow-text">SEED</span>-OPT<sub>2.7B</sub>.
                </div>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <br>
                <p>
                    The trained SEED-OPT<sub>2.7B</sub> is capable of both image captioning (image-to-text) and image generation (text-to-image).
                    More importantly, the model is able to perform open-ended Visual Question Answering (image&text-to-text), which can
                    be considered an <span style="background-color: rgb(253, 131, 131)">emergent ability</span>
                    as we did not use any VQA dataset for training. See the results below.
                </p>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <br>
                <h4>
                    Qualitative evaluation: image comprehension
                </h4>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="img/seed-opt-eval.png" class="img-responsive">
                    </div>
                </div>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div style="text-align: center; margin-top: 0px; margin-bottom: 24px; font-size: 17px;" class="seed">
                  Image captioning and open-ended visual question answering.
                </div>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <br>
                <h4>
                    Qualitative evaluation: image generation
                </h4>
            </div>
            <div class="col-md-8 col-md-offset-2">
                <div class="image-container">
                    <div class="image-wrapper">
                        <img src="img/seed-opt-eval-gen.png" class="img-responsive">
                    </div>
                </div>
            </div>
            <div class="col-md-10 col-md-offset-1">
                <div style="text-align: center; margin-top: 0px; margin-bottom: 24px; font-size: 17px;" class="seed">
                  Text-to-image generation.
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Citation</b>
                </h3>
                <div class="section bibtex">
<pre>
@article{ge2023planting,
  title={Planting a seed of vision in large language model},
  author={Ge, Yuying and Ge, Yixiao and Zeng, Ziyun and Wang, Xintao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.08041},
  year={2023}
}</pre>
              	  </div>
                  <p>
                    Get to know more about our <a target="_blank" href="./index.html">Project SEED</a>.
                  </p>

            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Acknowledgements</b>
                </h3>

                <p>
                  We sincerely acknowledge Sijie Zhao and Chen Li for their engaging discussions.
                </p>

                <p>
                    <br><br>
                    The website template was borrowed from <a target="_blank" href="https://robotics-transformer-x.github.io/">Open X-Embodiment</a>.
                </p>
            </div>
        </div>
    </div>
    </div>

    <script>
        const authors = document.querySelectorAll('.author');

        authors.forEach(author => {
            // Get the affiliation from the data-affiliation attribute
            const affiliation = author.getAttribute('data-affiliation');

            // Create a new span element with the class "affiliation"
            const span = document.createElement('span');
            span.className = 'affiliation';
            span.textContent = `${affiliation}`;

            // Append the span to the author element
            author.appendChild(span);
        });
    </script>
</body>

</html>
